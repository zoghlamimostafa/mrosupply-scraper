#!/usr/bin/env python3
"""
Smart Retry Manager for MRO Supply Scraper
Priority-based retry queue with exponential backoff
"""

import time
import heapq
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import defaultdict
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


@dataclass(order=True)
class RetryItem:
    """
    Item in retry queue

    Uses priority queue ordering based on:
    1. Priority (lower = higher priority)
    2. Next retry time (earlier = higher priority)
    """
    # Priority fields (used for ordering)
    priority: int
    next_retry_time: float

    # Data fields (not used for ordering)
    url: str = field(compare=False)
    attempt: int = field(compare=False)
    error_type: str = field(compare=False)
    error_message: str = field(compare=False)
    first_attempt_time: float = field(compare=False)
    metadata: Dict = field(default_factory=dict, compare=False)

    def __repr__(self) -> str:
        return (
            f"RetryItem(url={self.url[:50]}..., "
            f"priority={self.priority}, "
            f"attempt={self.attempt}, "
            f"error={self.error_type})"
        )


class SmartRetryManager:
    """
    Smart retry manager with priority queue

    Features:
    - Priority-based retry (rate limits first, 404s last)
    - Exponential backoff (1min, 2min, 4min, 8min, 16min)
    - Max 5 attempts per URL
    - Error categorization
    - Retry statistics
    """

    # Error priority levels (lower = higher priority)
    ERROR_PRIORITIES = {
        'rate_limit': 1,       # 429 - retry first
        'server_error': 2,     # 5xx - server issues
        'timeout': 3,          # Request timeout
        'connection': 4,       # Connection errors
        'client_error': 5,     # 4xx (except 429 and 404)
        'parse_error': 6,      # Parsing failures
        'validation': 7,       # Validation failures
        'not_found': 10,       # 404 - retry last
        'unknown': 8           # Unknown errors
    }

    # Base delay for exponential backoff (seconds)
    BASE_DELAYS = {
        'rate_limit': 60,      # Start with 1 minute
        'server_error': 30,    # 30 seconds
        'timeout': 30,         # 30 seconds
        'connection': 60,      # 1 minute
        'client_error': 120,   # 2 minutes
        'parse_error': 60,     # 1 minute
        'validation': 120,     # 2 minutes
        'not_found': 300,      # 5 minutes
        'unknown': 60          # 1 minute
    }

    MAX_ATTEMPTS = 5

    def __init__(self, config=None):
        """
        Initialize retry manager

        Args:
            config: Configuration object
        """
        self.config = config

        # Priority queue (heap)
        self.retry_queue: List[RetryItem] = []

        # URL tracking (prevent duplicates)
        self.urls_in_queue: set = set()

        # Statistics
        self.total_retries = 0
        self.successful_retries = 0
        self.failed_retries = 0
        self.error_counts = defaultdict(int)
        self.retry_counts_by_attempt = defaultdict(int)

        logger.info("Smart retry manager initialized")

    def add_retry(
        self,
        url: str,
        error_type: str,
        error_message: str = "",
        attempt: int = 1,
        metadata: Optional[Dict] = None
    ) -> bool:
        """
        Add URL to retry queue

        Args:
            url: URL that failed
            error_type: Type of error (used for priority)
            error_message: Error message
            attempt: Current attempt number
            metadata: Optional metadata

        Returns:
            bool: True if added, False if rejected (max attempts reached)
        """
        # Check if already at max attempts
        if attempt >= self.MAX_ATTEMPTS:
            logger.warning(
                f"Max retry attempts ({self.MAX_ATTEMPTS}) reached for {url}"
            )
            self.failed_retries += 1
            return False

        # Check if already in queue
        if url in self.urls_in_queue:
            logger.debug(f"URL already in retry queue: {url}")
            return False

        # Normalize error type
        error_type = self._normalize_error_type(error_type)

        # Get priority
        priority = self.ERROR_PRIORITIES.get(error_type, self.ERROR_PRIORITIES['unknown'])

        # Calculate next retry time (exponential backoff)
        base_delay = self.BASE_DELAYS.get(error_type, 60)
        delay = base_delay * (2 ** (attempt - 1))  # Exponential: 1x, 2x, 4x, 8x, 16x

        # Cap maximum delay
        delay = min(delay, 1800)  # Max 30 minutes

        next_retry_time = time.time() + delay

        # Create retry item
        item = RetryItem(
            priority=priority,
            next_retry_time=next_retry_time,
            url=url,
            attempt=attempt,
            error_type=error_type,
            error_message=error_message,
            first_attempt_time=time.time(),
            metadata=metadata or {}
        )

        # Add to queue
        heapq.heappush(self.retry_queue, item)
        self.urls_in_queue.add(url)

        # Update statistics
        self.total_retries += 1
        self.error_counts[error_type] += 1
        self.retry_counts_by_attempt[attempt] += 1

        logger.info(
            f"Added to retry queue: {url} "
            f"(priority={priority}, attempt={attempt}, delay={delay:.0f}s, "
            f"error={error_type})"
        )

        return True

    def _normalize_error_type(self, error_type: str) -> str:
        """Normalize error type to standard categories"""
        error_type_lower = error_type.lower()

        if '429' in error_type_lower or 'rate' in error_type_lower:
            return 'rate_limit'
        elif '5' in error_type_lower[:1] or 'server' in error_type_lower:
            return 'server_error'
        elif 'timeout' in error_type_lower:
            return 'timeout'
        elif 'connection' in error_type_lower or 'network' in error_type_lower:
            return 'connection'
        elif '404' in error_type_lower or 'not found' in error_type_lower:
            return 'not_found'
        elif '4' in error_type_lower[:1]:
            return 'client_error'
        elif 'parse' in error_type_lower:
            return 'parse_error'
        elif 'validation' in error_type_lower:
            return 'validation'
        else:
            return 'unknown'

    def get_next_batch(self, batch_size: int = 10) -> List[RetryItem]:
        """
        Get next batch of URLs ready to retry

        Args:
            batch_size: Maximum number of items to return

        Returns:
            List of RetryItems ready to retry
        """
        current_time = time.time()
        ready_items = []

        # Peek at items without removing
        temp_removed = []

        while len(ready_items) < batch_size and self.retry_queue:
            # Get highest priority item
            item = heapq.heappop(self.retry_queue)
            temp_removed.append(item)

            # Check if ready to retry
            if item.next_retry_time <= current_time:
                ready_items.append(item)
                self.urls_in_queue.discard(item.url)
            else:
                # Not ready yet, stop checking
                break

        # Put back items that weren't ready
        for item in temp_removed:
            if item not in ready_items:
                heapq.heappush(self.retry_queue, item)

        if ready_items:
            logger.info(f"Retrieved {len(ready_items)} URLs for retry")

        return ready_items

    def mark_retry_success(self, url: str):
        """Mark a retry as successful"""
        self.successful_retries += 1
        logger.info(f"Retry successful: {url}")

    def mark_retry_failed(self, url: str, error_type: str, error_message: str = ""):
        """
        Mark a retry as failed and re-add if attempts remain

        Args:
            url: URL that failed
            error_type: Type of error
            error_message: Error message
        """
        logger.warning(f"Retry failed: {url} - {error_type}")

        # Try to find original attempt count
        # (This is a limitation - in production, track attempts separately)
        # For now, just increment attempt
        # Note: The scraper should pass the attempt count

    def get_queue_size(self) -> int:
        """Get number of items in retry queue"""
        return len(self.retry_queue)

    def get_ready_count(self) -> int:
        """Get number of items ready to retry now"""
        current_time = time.time()
        return sum(1 for item in self.retry_queue if item.next_retry_time <= current_time)

    def get_next_retry_time(self) -> Optional[float]:
        """Get timestamp of next scheduled retry"""
        if not self.retry_queue:
            return None

        # Peek at top item without removing
        return self.retry_queue[0].next_retry_time

    def get_statistics(self) -> Dict:
        """
        Get retry statistics

        Returns:
            dict: Retry statistics
        """
        next_retry = self.get_next_retry_time()
        next_retry_in = None
        if next_retry:
            next_retry_in = max(0, next_retry - time.time())

        stats = {
            'queue_size': self.get_queue_size(),
            'ready_count': self.get_ready_count(),
            'total_retries': self.total_retries,
            'successful_retries': self.successful_retries,
            'failed_retries': self.failed_retries,
            'success_rate': (
                (self.successful_retries / self.total_retries * 100)
                if self.total_retries > 0 else 0
            ),
            'next_retry_in_seconds': next_retry_in,
            'error_counts': dict(self.error_counts),
            'retry_by_attempt': dict(self.retry_counts_by_attempt)
        }

        return stats

    def get_priority_breakdown(self) -> Dict:
        """Get breakdown of queue by priority"""
        priority_counts = defaultdict(int)

        for item in self.retry_queue:
            priority_counts[item.priority] += 1

        # Convert to named categories
        named_counts = {}
        for error_type, priority in self.ERROR_PRIORITIES.items():
            count = priority_counts.get(priority, 0)
            if count > 0:
                named_counts[error_type] = count

        return named_counts

    def clear_queue(self):
        """Clear entire retry queue"""
        count = len(self.retry_queue)
        self.retry_queue.clear()
        self.urls_in_queue.clear()
        logger.warning(f"Cleared {count} items from retry queue")

    def remove_url(self, url: str) -> bool:
        """
        Remove specific URL from queue

        Args:
            url: URL to remove

        Returns:
            bool: True if removed, False if not found
        """
        if url not in self.urls_in_queue:
            return False

        # Rebuild queue without this URL
        new_queue = [item for item in self.retry_queue if item.url != url]
        self.retry_queue = new_queue
        heapq.heapify(self.retry_queue)
        self.urls_in_queue.discard(url)

        logger.info(f"Removed {url} from retry queue")
        return True

    def export_failed_urls(self, filepath: str):
        """
        Export failed URLs to file

        Args:
            filepath: Path to output file
        """
        try:
            with open(filepath, 'w') as f:
                f.write("url,error_type,error_message,attempt,first_attempt_time\n")

                for item in sorted(self.retry_queue, key=lambda x: x.priority):
                    f.write(
                        f'"{item.url}","{item.error_type}","{item.error_message}",'
                        f'{item.attempt},{item.first_attempt_time}\n'
                    )

            logger.info(f"Exported {len(self.retry_queue)} failed URLs to {filepath}")

        except Exception as e:
            logger.error(f"Failed to export failed URLs: {e}")


if __name__ == '__main__':
    # Test retry manager
    print("Smart Retry Manager Test")
    print("=" * 60)

    manager = SmartRetryManager()

    # Add various error types
    print("\n1. Adding retry items with different priorities...")
    manager.add_retry("https://example.com/product/1", "rate_limit", "429 Too Many Requests", attempt=1)
    manager.add_retry("https://example.com/product/2", "timeout", "Request timeout", attempt=1)
    manager.add_retry("https://example.com/product/3", "not_found", "404 Not Found", attempt=1)
    manager.add_retry("https://example.com/product/4", "server_error", "500 Internal Server Error", attempt=1)
    manager.add_retry("https://example.com/product/5", "connection", "Connection refused", attempt=2)

    # Show statistics
    print("\n2. Retry queue statistics:")
    stats = manager.get_statistics()
    for key, value in stats.items():
        print(f"   {key}: {value}")

    # Show priority breakdown
    print("\n3. Priority breakdown:")
    breakdown = manager.get_priority_breakdown()
    for error_type, count in breakdown.items():
        priority = manager.ERROR_PRIORITIES[error_type]
        print(f"   {error_type} (priority {priority}): {count} items")

    # Simulate time passing (for testing, we'll just get items)
    print("\n4. Getting next batch (normally would wait for delay)...")
    print("   Note: In production, items won't be ready immediately due to delays")

    # Show next retry time
    next_retry = manager.get_next_retry_time()
    if next_retry:
        wait_time = next_retry - time.time()
        print(f"   Next retry in: {wait_time:.0f} seconds")

    print("\nâœ… Retry manager test completed")
